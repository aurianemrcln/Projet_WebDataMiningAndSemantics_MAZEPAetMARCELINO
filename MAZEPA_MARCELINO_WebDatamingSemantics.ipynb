{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import inflect\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from datasets import load_dataset\n",
    "\n",
    "import spacy\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Implementation (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional web scraping script with at least 10 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article():\n",
    "    text_article = \"Bonjour !\"\n",
    "    return text_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean text preprocessing (no HTML tags, proper sentence structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\auria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\auria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\auria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "p = inflect.engine()\n",
    "# convert number into words\n",
    "def convert_number(text):\n",
    "    # split string into list of words\n",
    "    temp_str = text.split()\n",
    "    # initialise empty list\n",
    "    new_string = []\n",
    "\n",
    "    for word in temp_str:\n",
    "        # if word is a digit, convert the digit\n",
    "        # to numbers and append into the new_string list\n",
    "        if word.isdigit():\n",
    "            temp = p.number_to_words(word)\n",
    "            new_string.append(temp)\n",
    "\n",
    "        # append the word as it is\n",
    "        else:\n",
    "            new_string.append(word)\n",
    "\n",
    "    # join the words of new_string to form a string\n",
    "    temp_str = ' '.join(new_string)\n",
    "    return temp_str\n",
    "\n",
    "def replace_non_alphabetic_with_whitespace(text):\n",
    "    modified_text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    return modified_text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet') \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def preprocess_pipeline(text):\n",
    "    text = text_lowercase(text)\n",
    "    text = convert_number(text)\n",
    "    text = replace_non_alphabetic_with_whitespace(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "\n",
    "    # Convertir la liste en chaîne après avoir supprimé les stopwords\n",
    "    text = \" \".join(remove_stopwords(text))\n",
    "    text = \" \".join(stem_words(text))\n",
    "    text = \" \".join(lemma_words(text))\n",
    "    \n",
    "    processed_example = {'tokens': text.split()}\n",
    "    return processed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['star', 'war', 'iv', 'movi', 'differ', 'kind', 'creatur', 'like', 'human', 'wooki', 'creatur', 'jedi', 'instanc', 'human', 'luke', 'jedi', 'master', 'yoda', 'speci', 'known', 'also', 'jedi', 'wooki', 'name', 'chewbacca', 'han', 'co', 'pilot', 'millennium', 'falcon', 'starship', 'speed', 'millennium', 'falcon', 'speed', 'light']}\n"
     ]
    }
   ],
   "source": [
    "text_star_wars = \"Star Wars IV is a Movie where there are different kinds of creatures, like humans and wookies. Some creatures are Jedis; for instance, the human Luke is a Jedi, and Master Yoda - for whom the species is not known - is also a Jedi. The wookie named Chewbacca is Han's co-pilot on the Millennium Falcon starship. The speed of Millennium Falcon is 1.5 (above the speed of light!)\"\n",
    "\n",
    "print(preprocess_pipeline(text_star_wars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete pipeline from scraping to graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "def pipeline_scrap_to_graph():\n",
    "    text_article = scrape_article()\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    displacy.render(nlp(text_article), style=\"dep\", jupyter=True, options={\"compact\": True, \"distance\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique entities identified (>50 entities: full points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation Extraction (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic relation extraction implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_from_text(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in [\"nsubj\", \"nsubjpass\"] and token.head.pos_ in [\"VERB\", \"AUX\", \"ROOT\"]:\n",
    "            subject = token.text\n",
    "            predicate = token.head.text\n",
    "            # Explorer les compléments pour identifier les objets et attributs\n",
    "            for child in token.head.children:\n",
    "                if child.dep_ in [\"dobj\", \"attr\", \"acomp\"]:\n",
    "                    relations.append((subject, predicate, child.text))\n",
    "                \n",
    "                elif child.dep_ == \"prep\" or child.dep_ == \"agent\":  # Gérer les relations prépositionnelles\n",
    "                    for obj in child.children:\n",
    "                        if obj.dep_ == \"pobj\":\n",
    "                            relations.append((subject, f\"{predicate} {child.text}\", obj.text))\n",
    "\n",
    "        # Gérer les relations attributives directes (ex: \"Luke is a Jedi\")\n",
    "        if token.dep_ in [\"attr\", \"appos\"] and token.head.dep_ in [\"nsubj\", \"nsubjpass\"]:\n",
    "            relations.append((token.head.text, \"is\", token.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At least 3 custom rules (with examples extracted from the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Graph Quality (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All triples in valid RDF format using proper namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At least 50 meaningful triples in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Linking (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working DBpedia/Wikidata linking implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > 50% entities linked to external knowledge bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentation of disambiguation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Graph Embedding (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training with at least 2 different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the performance is improved by data enhencement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Prediction (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation using standard metrics (MRR, Hits@k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
